"""
embed_words.py
è®€å– words.csv â†’ å‘¼å« OpenAI Embeddings â†’ å¯«å…¥ MySQL è³‡æ–™è¡¨ word_embeddings

å‰ç½®ï¼š
1. pip install openai pymysql python-dotenv pandas
2. æª”æ¡ˆçµæ§‹ï¼ˆä¾‹ï¼‰ï¼š
   backend/
     â”œâ”€ app.py
     â”œâ”€ embed_words.py   â† æ”¾é€™å€‹
     â””â”€ words.csv        â† ä½ çš„ 6000 å­— CSV

3. .env éœ€è¦ï¼š
   OPENAI_API_KEY=ä½ çš„é‡‘é‘°
   MYSQL_HOST=localhost
   MYSQL_USER=root
   MYSQL_PASSWORD=ï¼ˆä½ çš„å¯†ç¢¼ï¼‰
   MYSQL_DB=wordcrack
"""

import os
import json
import time
import traceback

import pandas as pd
import pymysql
from dotenv import load_dotenv
from openai import OpenAI

# -----------------------------
# è®€å– .env
# -----------------------------
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
MYSQL_HOST = os.getenv("MYSQL_HOST", "localhost")
MYSQL_USER = os.getenv("MYSQL_USER", "root")
MYSQL_PASSWORD = os.getenv("MYSQL_PASSWORD", "")
MYSQL_DB = os.getenv("MYSQL_DB", "wordcrack")

if not OPENAI_API_KEY:
    raise RuntimeError("âŒ æ²’æœ‰æ‰¾åˆ° OPENAI_API_KEYï¼Œè«‹åœ¨ .env è£¡è¨­å®š")

client = OpenAI(api_key=OPENAI_API_KEY)

# -----------------------------
# é€£ç·š MySQL
# -----------------------------
try:
    db = pymysql.connect(
        host=MYSQL_HOST,
        user=MYSQL_USER,
        password=MYSQL_PASSWORD,
        database=MYSQL_DB,
        charset="utf8mb4",
        cursorclass=pymysql.cursors.DictCursor,
        autocommit=False,  # æ‰¹æ¬¡ commit
    )
    print(f"âœ… å·²é€£ç·š MySQLï¼š{MYSQL_HOST} / DB={MYSQL_DB}")
except Exception:
    print("âŒ ç„¡æ³•é€£ç·š MySQLï¼š")
    traceback.print_exc()
    raise SystemExit(1)

# -----------------------------
# å»ºç«‹ word_embeddings è³‡æ–™è¡¨
# -----------------------------
CREATE_TABLE_SQL = """
CREATE TABLE IF NOT EXISTS word_embeddings (
  id INT AUTO_INCREMENT PRIMARY KEY,
  word_id INT NOT NULL,
  word VARCHAR(255) NOT NULL,
  embedding JSON NOT NULL,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  UNIQUE KEY uq_word_id (word_id),
  CONSTRAINT fk_word_embeddings_word
    FOREIGN KEY (word_id) REFERENCES words(id)
    ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
"""

with db.cursor() as cursor:
    cursor.execute(CREATE_TABLE_SQL)
db.commit()
print("âœ… å·²ç¢ºèªå»ºç«‹è³‡æ–™è¡¨ï¼šword_embeddings")

# -----------------------------
# è®€å– words.csv
# -----------------------------
CSV_PATH = "words.csv"

if not os.path.exists(CSV_PATH):
    raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ° {CSV_PATH}ï¼Œè«‹ç¢ºèªæª”æ¡ˆè·¯å¾‘")

df = pd.read_csv(CSV_PATH)

# ä½ çš„æ¬„ä½åç¨±ï¼šç´šåˆ¥, å–®å­—, å±¬æ€§, ä¸­æ–‡
# åšå€‹ä¿éšªï¼šå¦‚æœæœªä¾†ä½ æ”¹æˆè‹±æ–‡æ¬„ä½ä¹Ÿèƒ½ç”¨
col_level = "ç´šåˆ¥" if "ç´šåˆ¥" in df.columns else "level"
col_word = "å–®å­—" if "å–®å­—" in df.columns else "word"
col_pos = "å±¬æ€§" if "å±¬æ€§" in df.columns else "part_of_speech"
col_cn = "ä¸­æ–‡" if "ä¸­æ–‡" in df.columns else "chinese"

print(f"ğŸ“„ CSV ç¸½åˆ—æ•¸ï¼š{len(df)}")
print("ğŸ“Œ æ¬„ä½å°æ‡‰ï¼š", col_level, col_word, col_pos, col_cn)

# -----------------------------
# æº–å‚™è¦åš embedding çš„å–®å­—æ¸…å–®
# -----------------------------
to_embed = []

with db.cursor() as cursor:
    for _, row in df.iterrows():
        word = str(row[col_word]).strip()
        if not word:
            continue

        # æ‰¾å°æ‡‰ words è¡¨çš„ id
        cursor.execute(
            "SELECT id FROM words WHERE word = %s LIMIT 1",
            (word,),
        )
        r = cursor.fetchone()
        if not r:
            # å¦‚æœ DB è£¡æ²’æœ‰é€™å€‹ wordï¼Œå°±è·³éï¼ˆä½ ä¹Ÿå¯ä»¥é¸æ“‡å°å‡ºä¾†ï¼‰
            # print(f"âš ï¸ DB ä¸­æ‰¾ä¸åˆ°å–®å­—ï¼š{word}")
            continue

        word_id = r["id"]

        # æª¢æŸ¥æ˜¯å¦å·²ç¶“æœ‰ embeddingï¼Œé¿å…é‡è¤‡
        cursor.execute(
            "SELECT id FROM word_embeddings WHERE word_id = %s LIMIT 1",
            (word_id,),
        )
        exists = cursor.fetchone()
        if exists:
            # print(f"â­ å·²æœ‰ embeddingï¼Œç•¥éï¼š{word}")
            continue

        to_embed.append(
            {
                "word_id": word_id,
                "word": word,
            }
        )

print(f"ğŸ§® æº–å‚™ç”¢ç”Ÿ embeddings çš„å–®å­—æ•¸é‡ï¼š{len(to_embed)}")

if not to_embed:
    print("âœ… çœ‹èµ·ä¾†æ‰€æœ‰å–®å­—éƒ½å·²ç¶“æœ‰ embeddings äº†ï¼ŒçµæŸã€‚")
    db.close()
    raise SystemExit(0)

# -----------------------------
# å‘¼å« OpenAI Embeddings æ‰¹æ¬¡å¯«å…¥
# -----------------------------
BATCH_SIZE = 100
MODEL_NAME = "text-embedding-3-small"

total = len(to_embed)
processed = 0

try:
    with db.cursor() as cursor:
        for start in range(0, total, BATCH_SIZE):
            batch = to_embed[start : start + BATCH_SIZE]
            texts = [item["word"] for item in batch]

            print(f"ğŸš€ å‘¼å« embeddingsï¼š{start+1} ~ {start+len(batch)} / {total}")

            # å‘¼å« OpenAI Embeddings
            resp = client.embeddings.create(
                model=MODEL_NAME,
                input=texts,
            )

            # resp.data[i].embedding æ˜¯ä¸€å€‹ float list
            for item, emb_obj in zip(batch, resp.data):
                embedding = emb_obj.embedding  # list[float]
                cursor.execute(
                    """
                    INSERT INTO word_embeddings (word_id, word, embedding)
                    VALUES (%s, %s, %s)
                    ON DUPLICATE KEY UPDATE
                      embedding = VALUES(embedding)
                    """,
                    (
                        item["word_id"],
                        item["word"],
                        json.dumps(embedding),
                    ),
                )

            db.commit()
            processed += len(batch)
            print(f"âœ… å·²å¯«å…¥ {processed} / {total} ç­†")

            # é¿å…å¤ªå¿«ï¼ˆå¯èª¿æ•´æˆ–è¨»è§£æ‰ï¼‰
            time.sleep(0.2)

except Exception:
    print("âŒ ç”¢ç”Ÿ embeddings æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œæº–å‚™å›æ»¾äº¤æ˜“")
    db.rollback()
    traceback.print_exc()
finally:
    db.close()
    print("ğŸ”š çµæŸï¼Œè³‡æ–™åº«é€£ç·šå·²é—œé–‰")
